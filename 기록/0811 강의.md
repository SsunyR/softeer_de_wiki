# 0811 강의

## Optimizing Spark Jobs

- Parquet와 Spark 성능의 관계
  - Columnar Storage의 장점
    - 연산이 필요한 열만 처리할 수 있음
    - Schema Evolution이 비교적 쉬움

- Data Warehouse
  - 특정 목적을 가진 저장소(OLAP)
  - ETL
  - 정해진 구조

- Data Lake
  - 데이터를 일단 보관
  - ELT

- Data Lakehouse
  - Cloud
  - Data Warehouse + Data Lake
  - 두 장점을 모두 가지게끔

- Apache Parquet
  - for WORM
  - Non human readable

- Spark Job Optimizing
  - Spark Memory
    - Storage Memory 부족
      - Disk spilling이 일어남
    - Execution Memory 부족
  
    - Shuffle의 원인
      - Spark는 연산을 위한 데이터들이 같은 노드에 있을 것이라 예상
    - Partitioning 전략
      - Shuffle이 최소화되게끔
      - sortWithinPartitions & repartitionByRange

  - Caching
    - memory에 결과 저장
  - Persist
    - 필요하면 disk에도 저장
  - Checkpoint
    - 온전한 RDD 정보를 물리적으로 저장
    - Fault tolerance
  - Persist + Checkpoint
    - Persist를 통해 현재 Spark job에서 이득을, Checkpoint를 통해 다른 Spark job에서도 사용할 수 있음

  - Partitioning
    - repartition vs coalesce
    - 최적의 파티션 개수 vs 파티션 개수 축소

    - 파티션 개수 -> 코어 개수 + 데이터 분포 등 고려
    - 모든 파티션의 연산이 비슷한 속도가 되게끔

  - Splitting vs Salting

- Spark Web UI
  - Spark Driver Web UI
    - Live application의 정보
  - Spark History Server
    - application의 logs

- Shuffle Write size, Shuffle Write time
- Size in memory, Size on disk
  - 최적화를 하려면 데이터 구조를 알아야 함

### Optimize per DAG
