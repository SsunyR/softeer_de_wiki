{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad527b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/opt/homebrew/Cellar/apache-spark/4.0.0/libexec'\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/Cellar/openjdk@17/17.0.16/'\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55eca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81a2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/30 17:29:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"m2spark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('spark.driver.host', '127.0.0.1') \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.executor.cores', '8') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470a54a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>m2spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1389d6a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b088f",
   "metadata": {},
   "source": [
    "# 1. 데이터 로딩 및 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, expr, rand, mean, round\n",
    "\n",
    "FILE_NAME_HEADER = \"shared/data/fhv_tripdata_2024_1278/fhvhv_tripdata_2024-\"\n",
    "months = ['01', '01', '07', '08']\n",
    "fraction = 0.1\n",
    "sampled_df_all = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    file_path = f\"{FILE_NAME_HEADER}{month}.parquet\"\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    sampled_df = df.sample(False, fraction, seed=42)\n",
    "    \n",
    "    if sampled_df_all is None:\n",
    "        sampled_df_all = sampled_df\n",
    "    else:\n",
    "        sampled_df_all = sampled_df_all.union(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9804622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65005ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 불필요한 열 제거\n",
    "\n",
    "columns_to_drop = [\n",
    "    'dispatching_base_num', 'originating_base_num', 'shared_request_flag',\n",
    "    'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'\n",
    "]\n",
    "df_clean = sampled_df_all.drop(*columns_to_drop)\n",
    "\n",
    "# 3. 시간 조건 필터링\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"scene_ts\", unix_timestamp(\"on_scene_datetime\")) \\\n",
    "    .withColumn(\"request_ts\", unix_timestamp(\"request_datetime\"))\n",
    "\n",
    "df_clean = df_clean.filter((col(\"dropoff_ts\") > col(\"pickup_ts\")) &\n",
    "                           (col(\"scene_ts\") > col(\"request_ts\")) &\n",
    "                           (col(\"base_passenger_fare\") > 0) &\n",
    "                           (col(\"driver_pay\") >= 0))\n",
    "\n",
    "# scene_time 계산\n",
    "df_clean = df_clean.withColumn(\"scene_time\", expr(\"pickup_ts - scene_ts\"))\n",
    "\n",
    "# ts 열 제거\n",
    "df_clean = df_clean.drop(\"pickup_ts\", \"dropoff_ts\", \"scene_ts\", \"request_ts\")\n",
    "\n",
    "# 분단위 시간 변환\n",
    "# 반올림\n",
    "df_clean = df_clean.withColumn(\"scene_time\", round(expr(\"scene_time / 60\")))\n",
    "df_clean = df_clean.withColumn(\"trip_time\", round(expr(\"trip_time / 60\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0edb3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4f77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 정보 추출\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "df_with_date = df_clean \\\n",
    "    .withColumn(\"year\", year(\"request_datetime\")) \\\n",
    "    .withColumn(\"month\", month(\"request_datetime\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"request_datetime\")) \\\n",
    "    .withColumn(\"hour\", hour(\"request_datetime\"))\n",
    "\n",
    "# 계절 정보 추가\n",
    "df_with_date = df_with_date.withColumn(\n",
    "    \"season\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN month = 1 OR month = 2 THEN 'Winter'\n",
    "            WHEN month = 7 OR month = 8 THEN 'Summer'\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# 택시 라이센스 타입 추가\n",
    "df_with_date = df_with_date.withColumn(\n",
    "    \"service_type\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN hvfhs_license_num='HV0003' THEN 'Uber'\n",
    "            WHEN hvfhs_license_num='HV0004' THEN 'Lyft'\n",
    "        END\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c047a7",
   "metadata": {},
   "source": [
    "# 4. 지역 정보 로드 및 조인 (CSV -> Spark DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "514cd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone = spark.read.option(\"header\", True).csv(\n",
    "    \"/Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/taxi_zone_lookup.csv\"\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. PULocationID에 대한 join\n",
    "# df_with_date와 taxi_zone에 각각 별칭(alias)을 부여합니다.\n",
    "pu_zones = taxi_zone.alias(\"pu_zones\")\n",
    "\n",
    "df_with_pu = df_with_date.join(\n",
    "    pu_zones,\n",
    "    df_with_date.PULocationID == col(\"pu_zones.LocationID\"),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    df_with_date[\"*\"],\n",
    "    col(\"pu_zones.Borough\").alias(\"PULocation\") # 별칭을 사용해 명확히 지정\n",
    ")\n",
    "\n",
    "# 2. DOLocationID에 대한 join\n",
    "# taxi_zone에 다시 새로운 별칭을 부여합니다.\n",
    "do_zones = taxi_zone.alias(\"do_zones\")\n",
    "\n",
    "df_with_do = df_with_pu.join(\n",
    "    do_zones,\n",
    "    df_with_pu.DOLocationID == col(\"do_zones.LocationID\"),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    df_with_pu[\"*\"],\n",
    "    col(\"do_zones.Borough\").alias(\"DOLocation\") # 별칭을 사용해 명확히 지정\n",
    ")\n",
    "\n",
    "# drop locationid\n",
    "df_zone = df_with_do.drop(\"PULocationID\", \"DOLocationID\", \"LocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a4ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c31696",
   "metadata": {},
   "source": [
    "# 5. 분석 또는 저장용 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb7a3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.cache()  # 후속 분석을 위해 캐싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b81210",
   "metadata": {},
   "source": [
    "# 날씨 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa12b028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 17:29:12 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/2024_weather/*.csv.\n",
      "java.io.FileNotFoundException: File /Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/2024_weather/*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# 날씨 데이터 로드\n",
    "weather_path_header = \"/Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/2024_weather/\"\n",
    "weather_df = spark.read.option(\"header\", True).csv(weather_path_header + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1954e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, sum as spark_sum, round, expr\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "for col_name in ['precipitation1', 'precipitation2', 'precipitation3']:\n",
    "    weather_df = weather_df.withColumn(col_name, regexp_replace(col(col_name), 'T', '0').cast(\"float\"))\n",
    "\n",
    "weather_df = weather_df.withColumn(\n",
    "    \"precipitation\",\n",
    "    col(\"precipitation1\") + col(\"precipitation2\") + col(\"precipitation3\")\n",
    ")\n",
    "\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"max\", round((col(\"max\") - 32) * 5 / 9, 1)) \\\n",
    "    .withColumn(\"min\", round((col(\"min\") - 32) * 5 / 9, 1))\n",
    "\n",
    "weather_df = weather_df.drop(\"precipitation1\", \"precipitation2\", \"precipitation3\")\n",
    "\n",
    "\n",
    "df_final = df_zone.join(\n",
    "    weather_df,\n",
    "    on=[\"year\", \"month\", \"day\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdb7683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8069f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/30 17:29:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "{\"ts\": \"2025-07-30 17:29:12.392\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `max_C` cannot be resolved. Did you mean one of the following? [`max`, `bcf`, `day`, `min`, `hour`]. SQLSTATE: 42703\", \"context\": {\"file\": \"jdk.internal.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o238.select.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `max_C` cannot be resolved. Did you mean one of the following? [`max`, `bcf`, `day`, `min`, `hour`]. SQLSTATE: 42703;\\n'Project [year#103, month#104, hour#106, season#107, precipitation#167, 'max_C, 'min_C]\\n+- Project [year#103, month#104, day#105, hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137, max#168, ... 2 more fields]\\n   +- Join LeftOuter, (((cast(year#103 as bigint) = cast(year#155 as bigint)) AND (cast(month#104 as bigint) = cast(month#156 as bigint))) AND (cast(day#105 as bigint) = cast(day#157 as bigint)))\\n      :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137]\\n      :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, ... 1 more fields]\\n      :     +- Join LeftOuter, (cast(DOLocationID#8 as bigint) = cast(LocationID#132 as bigint))\\n      :        :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, Borough#127 AS PULocation#131]\\n      :        :  +- Join LeftOuter, (cast(PULocationID#7 as bigint) = cast(LocationID#126 as bigint))\\n      :        :     :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, CASE WHEN (hvfhs_license_num#0 = HV0003) THEN Uber WHEN (hvfhs_license_num#0 = HV0004) THEN Lyft END AS service_type#108]\\n      :        :     :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, CASE WHEN ((month#104 = 1) OR (month#104 = 2)) THEN Winter WHEN ((month#104 = 7) OR (month#104 = 8)) THEN Summer END AS season#107]\\n      :        :     :     +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour(request_datetime#3, Some(Asia/Seoul)) AS hour#106]\\n      :        :     :        +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, dayofmonth(cast(request_datetime#3 as date)) AS day#105]\\n      :        :     :           +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month(cast(request_datetime#3 as date)) AS month#104]\\n      :        :     :              +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year(cast(request_datetime#3 as date)) AS year#103]\\n      :        :     :                 +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, round((cast(trip_time#10L as double) / cast(60 as double)), 0) AS trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101]\\n      :        :     :                    +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, round((cast(scene_time#100L as double) / cast(60 as double)), 0) AS scene_time#101]\\n      :        :     :                       +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#100L]\\n      :        :     :                          +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, request_ts#99L, (pickup_ts#96L - scene_ts#98L) AS scene_time#100L]\\n      :        :     :                             +- Filter ((((dropoff_ts#97L > pickup_ts#96L) AND (scene_ts#98L > request_ts#99L)) AND (base_passenger_fare#11 > cast(0 as double))) AND (driver_pay#18 >= cast(0 as double)))\\n      :        :     :                                +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, unix_timestamp(request_datetime#3, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS request_ts#99L]\\n      :        :     :                                   +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, unix_timestamp(on_scene_datetime#4, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS scene_ts#98L]\\n      :        :     :                                      +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, unix_timestamp(dropoff_datetime#6, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS dropoff_ts#97L]\\n      :        :     :                                         +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, unix_timestamp(pickup_datetime#5, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS pickup_ts#96L]\\n      :        :     :                                            +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18]\\n      :        :     :                                               +- Union false, false\\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#0,dispatching_base_num#1,originating_base_num#2,request_datetime#3,on_scene_datetime#4,pickup_datetime#5,dropoff_datetime#6,PULocationID#7,DOLocationID#8,trip_miles#9,trip_time#10L,base_passenger_fare#11,tolls#12,bcf#13,sales_tax#14,congestion_surcharge#15,airport_fee#16,tips#17,driver_pay#18,shared_request_flag#19,shared_match_flag#20,access_a_ride_flag#21,wav_request_flag#22,wav_match_flag#23] parquet\\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#24,dispatching_base_num#25,originating_base_num#26,request_datetime#27,on_scene_datetime#28,pickup_datetime#29,dropoff_datetime#30,PULocationID#31,DOLocationID#32,trip_miles#33,trip_time#34L,base_passenger_fare#35,tolls#36,bcf#37,sales_tax#38,congestion_surcharge#39,airport_fee#40,tips#41,driver_pay#42,shared_request_flag#43,shared_match_flag#44,access_a_ride_flag#45,wav_request_flag#46,wav_match_flag#47] parquet\\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#48,dispatching_base_num#49,originating_base_num#50,request_datetime#51,on_scene_datetime#52,pickup_datetime#53,dropoff_datetime#54,PULocationID#55,DOLocationID#56,trip_miles#57,trip_time#58L,base_passenger_fare#59,tolls#60,bcf#61,sales_tax#62,congestion_surcharge#63,airport_fee#64,tips#65,driver_pay#66,shared_request_flag#67,shared_match_flag#68,access_a_ride_flag#69,wav_request_flag#70,wav_match_flag#71] parquet\\n      :        :     :                                                  +- Sample 0.0, 0.1, false, 42\\n      :        :     :                                                     +- Relation [hvfhs_license_num#72,dispatching_base_num#73,originating_base_num#74,request_datetime#75,on_scene_datetime#76,pickup_datetime#77,dropoff_datetime#78,PULocationID#79,DOLocationID#80,trip_miles#81,trip_time#82L,base_passenger_fare#83,tolls#84,bcf#85,sales_tax#86,congestion_surcharge#87,airport_fee#88,tips#89,driver_pay#90,shared_request_flag#91,shared_match_flag#92,access_a_ride_flag#93,wav_request_flag#94,wav_match_flag#95] parquet\\n      :        :     +- SubqueryAlias pu_zones\\n      :        :        +- Relation [LocationID#126,Borough#127,Zone#128,service_zone#129] csv\\n      :        +- SubqueryAlias do_zones\\n      :           +- Relation [LocationID#132,Borough#133,Zone#134,service_zone#135] csv\\n      +- Project [year#155, month#156, day#157, max#168, min#169, precipitation#167]\\n         +- Project [year#155, month#156, day#157, max#168, round((cast(((cast(min#159 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS min#169, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\\n            +- Project [year#155, month#156, day#157, round((cast(((cast(max#158 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS max#168, min#159, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\\n               +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, precipitation3#166, ((precipitation1#164 + precipitation2#165) + precipitation3#166) AS precipitation#167]\\n                  +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, cast(regexp_replace(precipitation3#162, T, 0, 1) as float) AS precipitation3#166]\\n                     +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, cast(regexp_replace(precipitation2#161, T, 0, 1) as float) AS precipitation2#165, precipitation3#162]\\n                        +- Project [year#155, month#156, day#157, max#158, min#159, cast(regexp_replace(precipitation1#160, T, 0, 1) as float) AS precipitation1#164, precipitation2#161, precipitation3#162]\\n                           +- Relation [year#155,month#156,day#157,max#158,min#159,precipitation1#160,precipitation2#161,precipitation3#162] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 21 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/admin/softeer_de_wiki/mission/W4/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/admin/softeer_de_wiki/mission/W4/.venv/lib/python3.11/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `max_C` cannot be resolved. Did you mean one of the following? [`max`, `bcf`, `day`, `min`, `hour`]. SQLSTATE: 42703;\n'Project [year#103, month#104, hour#106, season#107, precipitation#167, 'max_C, 'min_C]\n+- Project [year#103, month#104, day#105, hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137, max#168, ... 2 more fields]\n   +- Join LeftOuter, (((cast(year#103 as bigint) = cast(year#155 as bigint)) AND (cast(month#104 as bigint) = cast(month#156 as bigint))) AND (cast(day#105 as bigint) = cast(day#157 as bigint)))\n      :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137]\n      :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, ... 1 more fields]\n      :     +- Join LeftOuter, (cast(DOLocationID#8 as bigint) = cast(LocationID#132 as bigint))\n      :        :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, Borough#127 AS PULocation#131]\n      :        :  +- Join LeftOuter, (cast(PULocationID#7 as bigint) = cast(LocationID#126 as bigint))\n      :        :     :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, CASE WHEN (hvfhs_license_num#0 = HV0003) THEN Uber WHEN (hvfhs_license_num#0 = HV0004) THEN Lyft END AS service_type#108]\n      :        :     :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, CASE WHEN ((month#104 = 1) OR (month#104 = 2)) THEN Winter WHEN ((month#104 = 7) OR (month#104 = 8)) THEN Summer END AS season#107]\n      :        :     :     +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour(request_datetime#3, Some(Asia/Seoul)) AS hour#106]\n      :        :     :        +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, dayofmonth(cast(request_datetime#3 as date)) AS day#105]\n      :        :     :           +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month(cast(request_datetime#3 as date)) AS month#104]\n      :        :     :              +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year(cast(request_datetime#3 as date)) AS year#103]\n      :        :     :                 +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, round((cast(trip_time#10L as double) / cast(60 as double)), 0) AS trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101]\n      :        :     :                    +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, round((cast(scene_time#100L as double) / cast(60 as double)), 0) AS scene_time#101]\n      :        :     :                       +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#100L]\n      :        :     :                          +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, request_ts#99L, (pickup_ts#96L - scene_ts#98L) AS scene_time#100L]\n      :        :     :                             +- Filter ((((dropoff_ts#97L > pickup_ts#96L) AND (scene_ts#98L > request_ts#99L)) AND (base_passenger_fare#11 > cast(0 as double))) AND (driver_pay#18 >= cast(0 as double)))\n      :        :     :                                +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, unix_timestamp(request_datetime#3, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS request_ts#99L]\n      :        :     :                                   +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, unix_timestamp(on_scene_datetime#4, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS scene_ts#98L]\n      :        :     :                                      +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, unix_timestamp(dropoff_datetime#6, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS dropoff_ts#97L]\n      :        :     :                                         +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, unix_timestamp(pickup_datetime#5, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS pickup_ts#96L]\n      :        :     :                                            +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18]\n      :        :     :                                               +- Union false, false\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#0,dispatching_base_num#1,originating_base_num#2,request_datetime#3,on_scene_datetime#4,pickup_datetime#5,dropoff_datetime#6,PULocationID#7,DOLocationID#8,trip_miles#9,trip_time#10L,base_passenger_fare#11,tolls#12,bcf#13,sales_tax#14,congestion_surcharge#15,airport_fee#16,tips#17,driver_pay#18,shared_request_flag#19,shared_match_flag#20,access_a_ride_flag#21,wav_request_flag#22,wav_match_flag#23] parquet\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#24,dispatching_base_num#25,originating_base_num#26,request_datetime#27,on_scene_datetime#28,pickup_datetime#29,dropoff_datetime#30,PULocationID#31,DOLocationID#32,trip_miles#33,trip_time#34L,base_passenger_fare#35,tolls#36,bcf#37,sales_tax#38,congestion_surcharge#39,airport_fee#40,tips#41,driver_pay#42,shared_request_flag#43,shared_match_flag#44,access_a_ride_flag#45,wav_request_flag#46,wav_match_flag#47] parquet\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#48,dispatching_base_num#49,originating_base_num#50,request_datetime#51,on_scene_datetime#52,pickup_datetime#53,dropoff_datetime#54,PULocationID#55,DOLocationID#56,trip_miles#57,trip_time#58L,base_passenger_fare#59,tolls#60,bcf#61,sales_tax#62,congestion_surcharge#63,airport_fee#64,tips#65,driver_pay#66,shared_request_flag#67,shared_match_flag#68,access_a_ride_flag#69,wav_request_flag#70,wav_match_flag#71] parquet\n      :        :     :                                                  +- Sample 0.0, 0.1, false, 42\n      :        :     :                                                     +- Relation [hvfhs_license_num#72,dispatching_base_num#73,originating_base_num#74,request_datetime#75,on_scene_datetime#76,pickup_datetime#77,dropoff_datetime#78,PULocationID#79,DOLocationID#80,trip_miles#81,trip_time#82L,base_passenger_fare#83,tolls#84,bcf#85,sales_tax#86,congestion_surcharge#87,airport_fee#88,tips#89,driver_pay#90,shared_request_flag#91,shared_match_flag#92,access_a_ride_flag#93,wav_request_flag#94,wav_match_flag#95] parquet\n      :        :     +- SubqueryAlias pu_zones\n      :        :        +- Relation [LocationID#126,Borough#127,Zone#128,service_zone#129] csv\n      :        +- SubqueryAlias do_zones\n      :           +- Relation [LocationID#132,Borough#133,Zone#134,service_zone#135] csv\n      +- Project [year#155, month#156, day#157, max#168, min#169, precipitation#167]\n         +- Project [year#155, month#156, day#157, max#168, round((cast(((cast(min#159 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS min#169, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\n            +- Project [year#155, month#156, day#157, round((cast(((cast(max#158 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS max#168, min#159, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\n               +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, precipitation3#166, ((precipitation1#164 + precipitation2#165) + precipitation3#166) AS precipitation#167]\n                  +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, cast(regexp_replace(precipitation3#162, T, 0, 1) as float) AS precipitation3#166]\n                     +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, cast(regexp_replace(precipitation2#161, T, 0, 1) as float) AS precipitation2#165, precipitation3#162]\n                        +- Project [year#155, month#156, day#157, max#158, min#159, cast(regexp_replace(precipitation1#160, T, 0, 1) as float) AS precipitation1#164, precipitation2#161, precipitation3#162]\n                           +- Relation [year#155,month#156,day#157,max#158,min#159,precipitation1#160,precipitation2#161,precipitation3#162] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmonth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseason\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprecipitation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_C\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin_C\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/softeer_de_wiki/mission/W4/.venv/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:991\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> ParentDataFrame:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/softeer_de_wiki/mission/W4/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/softeer_de_wiki/mission/W4/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `max_C` cannot be resolved. Did you mean one of the following? [`max`, `bcf`, `day`, `min`, `hour`]. SQLSTATE: 42703;\n'Project [year#103, month#104, hour#106, season#107, precipitation#167, 'max_C, 'min_C]\n+- Project [year#103, month#104, day#105, hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137, max#168, ... 2 more fields]\n   +- Join LeftOuter, (((cast(year#103 as bigint) = cast(year#155 as bigint)) AND (cast(month#104 as bigint) = cast(month#156 as bigint))) AND (cast(day#105 as bigint) = cast(day#157 as bigint)))\n      :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, DOLocation#137]\n      :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, PULocation#131, ... 1 more fields]\n      :     +- Join LeftOuter, (cast(DOLocationID#8 as bigint) = cast(LocationID#132 as bigint))\n      :        :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, service_type#108, Borough#127 AS PULocation#131]\n      :        :  +- Join LeftOuter, (cast(PULocationID#7 as bigint) = cast(LocationID#126 as bigint))\n      :        :     :- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, season#107, CASE WHEN (hvfhs_license_num#0 = HV0003) THEN Uber WHEN (hvfhs_license_num#0 = HV0004) THEN Lyft END AS service_type#108]\n      :        :     :  +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour#106, CASE WHEN ((month#104 = 1) OR (month#104 = 2)) THEN Winter WHEN ((month#104 = 7) OR (month#104 = 8)) THEN Summer END AS season#107]\n      :        :     :     +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, day#105, hour(request_datetime#3, Some(Asia/Seoul)) AS hour#106]\n      :        :     :        +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month#104, dayofmonth(cast(request_datetime#3 as date)) AS day#105]\n      :        :     :           +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year#103, month(cast(request_datetime#3 as date)) AS month#104]\n      :        :     :              +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101, year(cast(request_datetime#3 as date)) AS year#103]\n      :        :     :                 +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, round((cast(trip_time#10L as double) / cast(60 as double)), 0) AS trip_time#102, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#101]\n      :        :     :                    +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, round((cast(scene_time#100L as double) / cast(60 as double)), 0) AS scene_time#101]\n      :        :     :                       +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, scene_time#100L]\n      :        :     :                          +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, request_ts#99L, (pickup_ts#96L - scene_ts#98L) AS scene_time#100L]\n      :        :     :                             +- Filter ((((dropoff_ts#97L > pickup_ts#96L) AND (scene_ts#98L > request_ts#99L)) AND (base_passenger_fare#11 > cast(0 as double))) AND (driver_pay#18 >= cast(0 as double)))\n      :        :     :                                +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, scene_ts#98L, unix_timestamp(request_datetime#3, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS request_ts#99L]\n      :        :     :                                   +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, dropoff_ts#97L, unix_timestamp(on_scene_datetime#4, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS scene_ts#98L]\n      :        :     :                                      +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, pickup_ts#96L, unix_timestamp(dropoff_datetime#6, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS dropoff_ts#97L]\n      :        :     :                                         +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18, unix_timestamp(pickup_datetime#5, yyyy-MM-dd HH:mm:ss, Some(Asia/Seoul), true) AS pickup_ts#96L]\n      :        :     :                                            +- Project [hvfhs_license_num#0, request_datetime#3, on_scene_datetime#4, pickup_datetime#5, dropoff_datetime#6, PULocationID#7, DOLocationID#8, trip_miles#9, trip_time#10L, base_passenger_fare#11, tolls#12, bcf#13, sales_tax#14, congestion_surcharge#15, airport_fee#16, tips#17, driver_pay#18]\n      :        :     :                                               +- Union false, false\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#0,dispatching_base_num#1,originating_base_num#2,request_datetime#3,on_scene_datetime#4,pickup_datetime#5,dropoff_datetime#6,PULocationID#7,DOLocationID#8,trip_miles#9,trip_time#10L,base_passenger_fare#11,tolls#12,bcf#13,sales_tax#14,congestion_surcharge#15,airport_fee#16,tips#17,driver_pay#18,shared_request_flag#19,shared_match_flag#20,access_a_ride_flag#21,wav_request_flag#22,wav_match_flag#23] parquet\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#24,dispatching_base_num#25,originating_base_num#26,request_datetime#27,on_scene_datetime#28,pickup_datetime#29,dropoff_datetime#30,PULocationID#31,DOLocationID#32,trip_miles#33,trip_time#34L,base_passenger_fare#35,tolls#36,bcf#37,sales_tax#38,congestion_surcharge#39,airport_fee#40,tips#41,driver_pay#42,shared_request_flag#43,shared_match_flag#44,access_a_ride_flag#45,wav_request_flag#46,wav_match_flag#47] parquet\n      :        :     :                                                  :- Sample 0.0, 0.1, false, 42\n      :        :     :                                                  :  +- Relation [hvfhs_license_num#48,dispatching_base_num#49,originating_base_num#50,request_datetime#51,on_scene_datetime#52,pickup_datetime#53,dropoff_datetime#54,PULocationID#55,DOLocationID#56,trip_miles#57,trip_time#58L,base_passenger_fare#59,tolls#60,bcf#61,sales_tax#62,congestion_surcharge#63,airport_fee#64,tips#65,driver_pay#66,shared_request_flag#67,shared_match_flag#68,access_a_ride_flag#69,wav_request_flag#70,wav_match_flag#71] parquet\n      :        :     :                                                  +- Sample 0.0, 0.1, false, 42\n      :        :     :                                                     +- Relation [hvfhs_license_num#72,dispatching_base_num#73,originating_base_num#74,request_datetime#75,on_scene_datetime#76,pickup_datetime#77,dropoff_datetime#78,PULocationID#79,DOLocationID#80,trip_miles#81,trip_time#82L,base_passenger_fare#83,tolls#84,bcf#85,sales_tax#86,congestion_surcharge#87,airport_fee#88,tips#89,driver_pay#90,shared_request_flag#91,shared_match_flag#92,access_a_ride_flag#93,wav_request_flag#94,wav_match_flag#95] parquet\n      :        :     +- SubqueryAlias pu_zones\n      :        :        +- Relation [LocationID#126,Borough#127,Zone#128,service_zone#129] csv\n      :        +- SubqueryAlias do_zones\n      :           +- Relation [LocationID#132,Borough#133,Zone#134,service_zone#135] csv\n      +- Project [year#155, month#156, day#157, max#168, min#169, precipitation#167]\n         +- Project [year#155, month#156, day#157, max#168, round((cast(((cast(min#159 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS min#169, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\n            +- Project [year#155, month#156, day#157, round((cast(((cast(max#158 as bigint) - cast(32 as bigint)) * cast(5 as bigint)) as double) / cast(9 as double)), 1) AS max#168, min#159, precipitation1#164, precipitation2#165, precipitation3#166, precipitation#167]\n               +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, precipitation3#166, ((precipitation1#164 + precipitation2#165) + precipitation3#166) AS precipitation#167]\n                  +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, precipitation2#165, cast(regexp_replace(precipitation3#162, T, 0, 1) as float) AS precipitation3#166]\n                     +- Project [year#155, month#156, day#157, max#158, min#159, precipitation1#164, cast(regexp_replace(precipitation2#161, T, 0, 1) as float) AS precipitation2#165, precipitation3#162]\n                        +- Project [year#155, month#156, day#157, max#158, min#159, cast(regexp_replace(precipitation1#160, T, 0, 1) as float) AS precipitation1#164, precipitation2#161, precipitation3#162]\n                           +- Relation [year#155,month#156,day#157,max#158,min#159,precipitation1#160,precipitation2#161,precipitation3#162] csv\n"
     ]
    }
   ],
   "source": [
    "df_final.select(\"year\", \"month\", \"hour\", \"season\", \"precipitation\", \"max_C\", \"min_C\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9089d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0e34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
