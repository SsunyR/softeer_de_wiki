{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad527b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/opt/homebrew/Cellar/apache-spark/4.0.0/libexec'\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/Cellar/openjdk@17/17.0.16/'\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55eca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"m2spark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config('spark.driver.host', '127.0.0.1') \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.executor.cores', '8') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b088f",
   "metadata": {},
   "source": [
    "# 1. 데이터 로딩 및 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, expr, rand, mean, round\n",
    "\n",
    "FILE_NAME_HEADER = \"shared/data/fhv_tripdata_2024_1278/fhvhv_tripdata_2024-\"\n",
    "months = ['01', '01', '07', '08']\n",
    "fraction = 0.1\n",
    "sampled_df_all = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    file_path = f\"{FILE_NAME_HEADER}{month}.parquet\"\n",
    "    df = spark.read.parquet(file_path)\n",
    "    \n",
    "    sampled_df = df.sample(False, fraction, seed=42)\n",
    "    \n",
    "    if sampled_df_all is None:\n",
    "        sampled_df_all = sampled_df\n",
    "    else:\n",
    "        sampled_df_all = sampled_df_all.union(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9804622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65005ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 불필요한 열 제거\n",
    "\n",
    "columns_to_drop = [\n",
    "    'dispatching_base_num', 'originating_base_num', 'shared_request_flag',\n",
    "    'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'\n",
    "]\n",
    "df_clean = sampled_df_all.drop(*columns_to_drop)\n",
    "\n",
    "# 3. 시간 조건 필터링\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"scene_ts\", unix_timestamp(\"on_scene_datetime\")) \\\n",
    "    .withColumn(\"request_ts\", unix_timestamp(\"request_datetime\"))\n",
    "\n",
    "df_clean = df_clean.filter((col(\"dropoff_ts\") > col(\"pickup_ts\")) &\n",
    "                           (col(\"scene_ts\") > col(\"request_ts\")) &\n",
    "                           (col(\"base_passenger_fare\") > 0) &\n",
    "                           (col(\"driver_pay\") >= 0))\n",
    "\n",
    "# scene_time 계산\n",
    "df_clean = df_clean.withColumn(\"scene_time\", expr(\"pickup_ts - scene_ts\"))\n",
    "\n",
    "# ts 열 제거\n",
    "df_clean = df_clean.drop(\"pickup_ts\", \"dropoff_ts\", \"scene_ts\", \"request_ts\")\n",
    "\n",
    "# 분단위 시간 변환\n",
    "# 반올림\n",
    "df_clean = df_clean.withColumn(\"scene_time\", round(expr(\"scene_time / 60\")))\n",
    "df_clean = df_clean.withColumn(\"trip_time\", round(expr(\"trip_time / 60\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 정보 추출\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "df_with_date = df_clean \\\n",
    "    .withColumn(\"year\", year(\"request_datetime\")) \\\n",
    "    .withColumn(\"month\", month(\"request_datetime\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(\"request_datetime\")) \\\n",
    "    .withColumn(\"hour\", hour(\"request_datetime\"))\n",
    "\n",
    "# 계절 정보 추가\n",
    "df_with_date = df_with_date.withColumn(\n",
    "    \"season\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN month = 1 OR month = 2 THEN 'Winter'\n",
    "            WHEN month = 7 OR month = 8 THEN 'Summer'\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# 택시 라이센스 타입 추가\n",
    "df_with_date = df_with_date.withColumn(\n",
    "    \"service_type\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN hvfhs_license_num='HV0003' THEN 'Uber'\n",
    "            WHEN hvfhs_license_num='HV0004' THEN 'Lyft'\n",
    "        END\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ebf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c047a7",
   "metadata": {},
   "source": [
    "# 4. 지역 정보 로드 및 조인 (CSV -> Spark DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone = spark.read.option(\"header\", True).csv(\n",
    "    \"/Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/taxi_zone_lookup.csv\"\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. PULocationID에 대한 join\n",
    "# df_with_date와 taxi_zone에 각각 별칭(alias)을 부여합니다.\n",
    "pu_zones = taxi_zone.alias(\"pu_zones\")\n",
    "\n",
    "df_with_pu = df_with_date.join(\n",
    "    pu_zones,\n",
    "    df_with_date.PULocationID == col(\"pu_zones.LocationID\"),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    df_with_date[\"*\"],\n",
    "    col(\"pu_zones.Borough\").alias(\"PULocation\") # 별칭을 사용해 명확히 지정\n",
    ")\n",
    "\n",
    "# 2. DOLocationID에 대한 join\n",
    "# taxi_zone에 다시 새로운 별칭을 부여합니다.\n",
    "do_zones = taxi_zone.alias(\"do_zones\")\n",
    "\n",
    "df_with_do = df_with_pu.join(\n",
    "    do_zones,\n",
    "    df_with_pu.DOLocationID == col(\"do_zones.LocationID\"),\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    df_with_pu[\"*\"],\n",
    "    col(\"do_zones.Borough\").alias(\"DOLocation\") # 별칭을 사용해 명확히 지정\n",
    ")\n",
    "\n",
    "# drop locationid\n",
    "df_zone = df_with_do.drop(\"PULocationID\", \"DOLocationID\", \"LocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c31696",
   "metadata": {},
   "source": [
    "# 5. 분석 또는 저장용 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.cache()  # 후속 분석을 위해 캐싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b81210",
   "metadata": {},
   "source": [
    "# 날씨 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨 데이터 로드\n",
    "weather_path_header = \"/Users/admin/softeer_de_wiki/mission/W4/m2/shared/data/2024_weather/\"\n",
    "weather_df = spark.read.option(\"header\", True).csv(weather_path_header + \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, sum as spark_sum, round, expr\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "for col_name in ['precipitation1', 'precipitation2', 'precipitation3']:\n",
    "    weather_df = weather_df.withColumn(col_name, regexp_replace(col(col_name), 'T', '0').cast(\"float\"))\n",
    "\n",
    "weather_df = weather_df.withColumn(\n",
    "    \"precipitation\",\n",
    "    col(\"precipitation1\") + col(\"precipitation2\") + col(\"precipitation3\")\n",
    ")\n",
    "\n",
    "weather_df = weather_df \\\n",
    "    .withColumn(\"max\", round((col(\"max\") - 32) * 5 / 9, 1)) \\\n",
    "    .withColumn(\"min\", round((col(\"min\") - 32) * 5 / 9, 1))\n",
    "\n",
    "weather_df = weather_df.drop(\"precipitation1\", \"precipitation2\", \"precipitation3\")\n",
    "\n",
    "\n",
    "df_final = df_zone.join(\n",
    "    weather_df,\n",
    "    on=[\"year\", \"month\", \"day\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8069f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.select(\"year\", \"month\", \"hour\", \"season\", \"precipitation\", \"max_C\", \"min_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9089d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0e34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
