# 2025 08.18 리뷰 & 회고

## 강의 리뷰

### Catalyst Optimizer

- Rule Based Optimizer
  - Analise with Catalog
  - Logical Plan
  - Physical Plan with Tungsten

- Physical Plan Generation
  - 하나의 Optimized Logical Plan에 대해 여러 개의 Physical Plan
    - Data 모양에 따라 Cost가 달라질 가능성이 있어 비교해 가장 적은 비용으로
    - Data 모양은 어디서?
- Code Generation
  - Whole-Stage Code Generation: 스테이지 단위로 함수를 통으로 제작
  - 장점: 스테이지별 관리가 가능 = shuffle 추적이 용이하다.

- Dynamic Partition Pruning
  - Columnar Data라서 Column의 추가와 삭제가 가능
  - *처리 데이터의 크기를 줄이는게 목적* = 모든 optimizing의 목적

### Adaptive Query Execution

- *Dynamic*
- 실행시간에 데이터 통계를 보고 실행계획을 동적 튜닝
  - Data Skew: 한 노드가 다른 노드에 비해 현저히 느림
- 기본적으로 Storage와 Computing layer는 분리됨
  - 데이터의 모양을 실행시간에 판단할 수 밖에 없음.
- *Query Stage*
  - MapOutputStatistics -> runtime statistics 제공

- *Logical Plan*을 수정

### Parallelism on Reducers

- Post-Shuffle Partitions

### Join Strategy

- shuffle이 거의 무조건 일어남 -> 성능에 큰 영향

- ShuffleHashJoin
  - Dynamic Filter
  - Hash table 크기가 작아서 메모리에 다 들어갈 수 있을 때
- SortMergeJoin
  - reduce comparation
- BroadcastHashJoin
  - Hash table 크기가 작아서 메모리에 다 들어갈 수 있을 때
  - Based on Rule
- Skewed Join

## AQE Not Needed

- Runtime statistics overhead 시간이 Job execution 시간과 별 차이가 없는 작업
- Well Optimized Jobs

### Join Followed by a Shuffle

- Join efficiency

### Shuffling After a Join

- Not always

### 결론

- Spark Optimizer가 수행하지 못한 최적화는 사람이 해야됨.
  - Spark Executor를 보고 문제를 인지할 수 있어야 함
  - Spark Optimizer가 담당하는 최적화 작업을 알아야 문제를 파악할 수 있음
  - Data Engineer에게도 Data Analysis 능력이 필요

## 회고(구체적인 행동, 어떻게 그런 행동을 하게 됐는지)

### Keep

- 이슈가 있을 땐 바로 팀원에게 전달한다. 이해를 빠르게 돕기 위해 글로 쓰거나 그림으로 그린다.

### Problem(해결방안과 함께)

- 

### Try('왜?'와 함께)

- 남은 시간이 절대적으로 많지 않기 때문에 시간을 영리하게 써야한다. -> 역할을 분담해 Agile하게 진행하면서 해결 못한 이슈는 빠르게 팀원에게 전달해 해결방법을 강구, 우선 해결된 부분에 대해서 전체 파이프라인을 먼저 구성하고 수정한다.
